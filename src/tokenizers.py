import os

from tokenizers import Tokenizer
from tokenizers.models import BPE, Unigram, WordLevel, WordPiece
from tokenizers.pre_tokenizers import Whitespace
from tokenizers.trainers import BpeTrainer, UnigramTrainer, WordLevelTrainer, WordPieceTrainer
from transformers import PreTrainedTokenizerFast

# TODO: Replace example, generated by AI token txt files with actual ones
# ! Current Data for token is AI


def bpe_tokenizer(
    training_directory: str = "data/training/",
    output_file_directory: str = "data/BPE_tokenizer.json",
) -> PreTrainedTokenizerFast:
    if not (training_directory.endswith("/")):
        training_directory += "/"

    tokenizer = Tokenizer(BPE(unk_token="[UNK]"))
    trainer = BpeTrainer(
        special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"]
    )  # Adds other tokens like unknown, padding, mask, and other post processing tokens
    tokenizer.pre_tokenizer = Whitespace()  # Necessary to avoid \n in tokens

    files = [os.path.join(training_directory, file) for file in os.listdir(training_directory)]
    tokenizer.train(files, trainer)

    tokenizer.save(output_file_directory)  # Saves to token.json

    tokenizer = PreTrainedTokenizerFast(tokenizer_file=output_file_directory)

    return tokenizer


def wordpiece_tokenizer(
    training_directory: str = "data/training/",
    output_file_directory: str = "data/wordpiece_tokenizer.json",
) -> PreTrainedTokenizerFast:
    if not (training_directory.endswith("/")):
        training_directory += "/"

    tokenizer = Tokenizer(WordPiece(unk_token="[UNK]"))
    trainer = WordPieceTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])
    tokenizer.pre_tokenizer = Whitespace()

    files = [os.path.join(training_directory, file) for file in os.listdir(training_directory)]
    tokenizer.train(files, trainer)

    tokenizer.save(output_file_directory)

    tokenizer = PreTrainedTokenizerFast(tokenizer_file=output_file_directory)

    return tokenizer


def unigram_tokenizer(
    training_directory: str = "data/training/",
    output_file_directory: str = "data/unigram_tokenizer.json",
) -> PreTrainedTokenizerFast:
    if not (training_directory.endswith("/")):
        training_directory += "/"

    tokenizer = Tokenizer(Unigram())
    trainer = UnigramTrainer(
        special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"]
    )  # Adds other tokens like unknown, padding, mask, and other post processing tokens
    tokenizer.pre_tokenizer = Whitespace()  # Necessary to avoid \n in tokens

    files = [os.path.join(training_directory, file) for file in os.listdir(training_directory)]
    tokenizer.train(files, trainer)

    tokenizer.save(output_file_directory)  # Saves to token.json

    tokenizer = PreTrainedTokenizerFast(tokenizer_file=output_file_directory)

    return tokenizer


def wordlevel_tokenizer(
    training_directory: str = "data/training/",
    output_file_directory: str = "data/wordlevel_tokenizer.json",
) -> PreTrainedTokenizerFast:
    if not (training_directory.endswith("/")):
        training_directory += "/"

    tokenizer = Tokenizer(WordLevel(unk_token="[UNK]"))
    trainer = WordLevelTrainer(
        special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"]
    )  # Adds other tokens like unknown, padding, mask, and other post processing tokens
    tokenizer.pre_tokenizer = Whitespace()  # Necessary to avoid \n in tokens

    files = [os.path.join(training_directory, file) for file in os.listdir(training_directory)]
    tokenizer.train(files, trainer)

    tokenizer.save(output_file_directory)  # Saves to token.json

    tokenizer = PreTrainedTokenizerFast(tokenizer_file=output_file_directory)

    return tokenizer
