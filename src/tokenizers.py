import os
from typing import Literal

from tokenizers import Tokenizer
from tokenizers.models import BPE, Unigram, WordLevel, WordPiece
from tokenizers.pre_tokenizers import Whitespace
from tokenizers.trainers import BpeTrainer, UnigramTrainer, WordLevelTrainer, WordPieceTrainer
from transformers import PreTrainedTokenizerFast

# TODO: Replace example, generated by AI token txt files with actual ones
# ! Current Data for token is AI


def train_tokenizer(
    type: Literal["bpe", "wordpiece", "unigram", "wordlevel"],
    training_directory: str = "data/training/",
    output_file_directory: str = "data/tokenizer.json",
    vocab_size: int = 5000,
):
    if not (training_directory.endswith("/")):
        training_directory += "/"

    if type == "bpe":
        tokenizer = Tokenizer(BPE(unk_token="[UNK]"))

        trainer = BpeTrainer(
            special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"], vocab_size=vocab_size
        )  # Adds other tokens like unknown, padding, mask, and other post processing tokens
        tokenizer.pre_tokenizer = Whitespace()  # Necessary to avoid \n in tokens
    elif type == "wordpiece":
        tokenizer = Tokenizer(WordPiece(unk_token="[UNK]"))

        trainer = WordPieceTrainer(
            special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"], vocab_size=vocab_size
        )  # Adds other tokens like unknown, padding, mask, and other post processing tokens
    elif type == "unigram":
        tokenizer = Tokenizer(Unigram())

        trainer = UnigramTrainer(
            special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"], vocab_size=vocab_size
        )  # Adds other tokens like unknown, padding, mask, and other post processing tokens

    elif type == "wordlevel":
        tokenizer = Tokenizer(WordLevel(unk_token="[UNK]"))

        trainer = WordLevelTrainer(
            special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"], vocab_size=vocab_size
        )  # Adds other tokens like unknown, padding, mask, and other post processing tokens

    tokenizer.pre_tokenizer = Whitespace()  # Necessary to avoid \n in tokens

    files = [os.path.join(training_directory, file) for file in os.listdir(training_directory)]
    tokenizer.train(files, trainer)

    tokenizer.save(output_file_directory)  # Saves to token.json

    tokenizer = PreTrainedTokenizerFast(tokenizer_file=output_file_directory)

    return tokenizer
